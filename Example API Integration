Example API Integration code for this project:
import pandas as pd
import scipy.stats as st
from dash import Dash, dcc, html, Input, Output
import plotly.express as px
import pprint
import json
import urllib.request
import time

measurements_url = 'https://data.cnra.ca.gov/api/3/action/datastore_search?resource_id=bfa9f262-24a1-45bd-8dc8-138bc8107266&limit=5&q='  
fileobj = urllib.request.urlopen(measurements_url)
measurements_response_dict = json.loads(fileobj.read())
pprint.pprint(measurements_response_dict)

stations_url = 'https://data.cnra.ca.gov/api/3/action/datastore_search?resource_id=af157380-fb42-4abf-b72a-6f9f98868077&limit=5&q='  
fileobj = urllib.request.urlopen(stations_url)
stations_response_dict = json.loads(fileobj.read())
pprint.pprint(stations_response_dict)

#TEST pull:
#Define the URLs for measurements and stations data
measurements_url = 'https://data.cnra.ca.gov/api/3/action/datastore_search?resource_id=bfa9f262-24a1-45bd-8dc8-138bc8107266&q='
stations_url = 'https://data.cnra.ca.gov/api/3/action/datastore_search?resource_id=af157380-fb42-4abf-b72a-6f9f98868077'

#Function to fetch records from an API endpoint with pauses between requests
def fetch_records_with_pause(url, pause_duration=1.0, chunk_size=100, num_pages=2):
    all_records = []
    for page in range(num_pages):
        #Construct the URL with pagination parameters
        url_with_pagination = f'{url}&limit={chunk_size}&offset={page * chunk_size}'
        with urllib.request.urlopen(url_with_pagination) as fileobj:
            response = json.load(fileobj)
            records = response['result']['records']
            all_records.extend(records)
        time.sleep(pause_duration)  # Introduce a pause between requests
    return all_records

# Fetch records from measurements and stations APIs for testing
measurements_records = fetch_records_with_pause(measurements_url)
stations_records = fetch_records_with_pause(stations_url)

# Initialize an empty list to store formatted measurements data
formatted_measurements_data = []

# Iterate over each record in measurements data and extract required fields
for record in measurements_records:
    formatted_measurements_data.append({
        'Site Code': record['site_code'],
        'Measurement Date': record['msmt_date'],
        'Groundwater Elevation': record['gwe'],
        'Quality': record['wlm_qa_desc']
    })

# Initialize an empty list to store formatted stations data
formatted_stations_data = []

# Iterate over each record in stations data and extract required fields
for record in stations_records:
    formatted_stations_data.append({
        'Site Code': record['site_code'],
        'Latitude': record['latitude'],
        'Longitude': record['longitude']
    })

# Convert the formatted data into DataFrames
measurements_df = pd.DataFrame(formatted_measurements_data)
stations_df = pd.DataFrame(formatted_stations_data)

# Merge DataFrames on 'site_code'
test_merged_df = pd.merge(measurements_df, stations_df, on='Site Code', how='left')

# Display the merged DataFrame
test_merged_df

# Function to fetch all wanted records from API endpoint with pauses between requests
def retrieve_with_pause(url, pause_duration=1.0, chunk_size=100):
    all_records = []
    offset = 0
    try:
        while True:
            # Construct the URL with pagination parameters
            url_through_pages = f'{url}&limit={chunk_size}&offset={offset}'
            with urllib.request.urlopen(url_through_pages) as fileobj:
                response = json.load(fileobj)
                if 'result' in response and 'records' in response['result']:
                    records = response['result']['records']
                    if not records:
                        break
                    all_records.extend(records)
                    offset += chunk_size
                else:
                    print("Error: Unexpected response format")
                    break
            time.sleep(pause_duration)  # Introduce a pause between requests
    except Exception as e:
        print(f"Error fetching data: {e}")
    return all_records

# Define the URLs for measurements and stations data
measurements_url = 'https://data.cnra.ca.gov/api/3/action/datastore_search?resource_id=bfa9f262-24a1-45bd-8dc8-138bc8107266&q='
stations_url = 'https://data.cnra.ca.gov/api/3/action/datastore_search?resource_id=af157380-fb42-4abf-b72a-6f9f98868077'

# Fetch all measurements and stations records with pauses between requests
measurements_records = retrieve_with_pause(measurements_url)
stations_records = retrieve_with_pause(stations_url)

# Initialize an empty list to store formatted measurements data
measurements_data = []

# Iterate over each record in measurements data and extract required fields
for record in measurements_records:
    measurements_data.append({
        'Site Code': record['site_code'],
        'Measurement Date': record['msmt_date'],
        'Groundwater Elevation': record['gwe'],
        'Quality': record.get('wlm_qa_desc', '')  # Use get method with default value to handle missing keys
    })

# Initialize an empty list to store formatted stations data
stations_data = []

# Iterate over each record in stations data and extract required fields
for record in stations_records:
    stations_data.append({
        'Site Code': record['site_code'],
        'Latitude': record['latitude'],
        'Longitude': record['longitude']
    })

# Convert the formatted data into DataFrames
measurements_df = pd.DataFrame(measurements_data)
stations_df = pd.DataFrame(stations_data)

# Merge DataFrames on 'site_code'
fin_df = pd.merge(measurements_df, stations_df, on='Site Code', how='left')

# Display the merged DataFrame
fin_df.head(20)





